# Architecture Documentation for Azure OpenAI and RAG-Based Solution

## Import Required Libraries
from IPython.display import Image, display

# Display the architecture diagram
display(Image(filename='awb_ai_app.png'))


## Overview
This architecture integrates **Azure OpenAI**, **Retrieval-Augmented Generation (RAG)**, and a data ingestion pipeline to process, store, and retrieve information efficiently. Below is a detailed explanation of each component and its role in the system.

---

### 1. **Components**

#### **1.1 Easy IDA**
- A user-facing interface for authentication and access control.
- Ensures secure interaction with the system via **Route53 DNS**.

#### **1.2 Web Application**
- Acts as the primary interface for users to interact with the system.
- Accepts user inputs (prompts) and sends them to downstream components for processing.

#### **1.3 LangChain**
- A framework used to manage prompt engineering.
- Generates different types of prompts based on user personas:
  - **Original Prompt**: Direct input from the user.
  - **Prompt Based on Persona**: Tailored prompts based on user type (e.g., technical or humorous).

#### **1.4 Azure OpenAI**
- Processes prompts using AI models hosted on Azure.
- Supports natural language understanding and generation tasks.

#### **1.5 Retrieval-Augmented Generation (RAG)**
- Enhances AI responses by retrieving relevant context from external knowledge bases.
- Interacts with:
  - **Open Search Vector DB**: Stores vectorized representations of ingested data for efficient retrieval.
  - **ElasticCache**: Maintains chat history to provide context in ongoing conversations.

#### **1.6 Data Ingestion App**
- Responsible for ingesting unstructured data (e.g., text from Confluence).
- Converts raw data into vectorized format for storage in Open Search Vector DB.

---

### 2. **Workflow**

#### **Step 1: User Interaction**
- Users interact with the system via the web application.
- Prompts are sent to LangChain for processing.

#### **Step 2: Prompt Engineering**
- LangChain generates tailored prompts based on:
  - User persona (e.g., technical or humorous).
  - Original input from the user.

#### **Step 3: AI Processing**
- Prompts are sent to Azure OpenAI for natural language processing.
- Responses are enhanced using RAG by retrieving relevant information from:
  - Open Search Vector DB (contextual data).
  - ElasticCache (chat history).

#### **Step 4: Data Ingestion**
- The Data Ingestion App processes external data sources (e.g., Confluence text).
- Stores vectorized representations in Open Search Vector DB for future retrieval.

---

### 3. **Key Technologies**

| Component              | Technology Used          |
|------------------------|--------------------------|
| Authentication         | Easy IDA, Route53 DNS   |
| Web Application        | Streamlit               |
| Prompt Engineering     | LangChain               |
| AI Processing          | Azure OpenAI            |
| Retrieval              | RAG                     |
| Storage                | Open Search Vector DB, ElasticCache |
| Data Ingestion         | Custom Data Ingestion App |

---

### 4. **Benefits of This Architecture**

1. **Scalability**: Cloud-based components like Azure OpenAI and AWS services ensure scalability.
2. **Efficiency**: RAG enhances response quality by providing relevant context.
3. **Flexibility**: LangChain allows dynamic prompt generation based on user needs.
4. **Data Utilization**: The ingestion pipeline enables effective use of external knowledge sources.

---

### Conclusion

This architecture demonstrates a robust integration of AI capabilities with retrieval-based enhancements, making it suitable for applications requiring intelligent, context-aware responses.
